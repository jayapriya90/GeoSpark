
## How to get started (For Scala and Java developers)

### Prerequisites
1. Apache Hadoop 2.4.0 and later
2. Apache Spark 1.2.1 and later
3. JDK 1.7

### Compilation Steps
git clone https://github.com/jayapriya90/GeoSpark.git
1. cd GeoSpark
2. mvn compile
3. mvn package

This will create a GeoSpark-0.1.jar file in the target folder.

### Steps to deploy
1. Create your own Apache Spark project
2. Add GeoSpark.jar into your Apache Spark build environment
3. You can now use GeoSpark spatial RDDs in your Apache Spark program to store spatial data and call needed functions!

